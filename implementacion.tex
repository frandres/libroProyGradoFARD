\chapter{Detalles de implementación} \label{chap:implementacion}

En este capítulo se harán algunos comentarios pertinentes sobre la implementación de la solución diseñada. Primeramente se expondrán algunas consideraciones generales sobre las tecnologías utilizadas; posteriormente se darán detalles sobre la implementación de cada uno de los módulos propuestos en la solución. 

El lenguaje de programación utilizado para la implementación del sistema de Extracción Focalizada de Información fue Java en su versión 1.6.0\_20. \\

Se utilizaron los siguientes paquetes de software \emph{open source}: 

\shadowbox{Cambié librería por paquete de software. Es necesario definir open source?}\\

\begin{itemize}
\item Para desplegar mensajes de traza y errores se utilizó el paquete Log4J. Dicho paquete permite en lugar de imprimir mensajes por la salida estándar, imprimirlos dependiendo de un nivel de desarrollo especificado en un archivo de configuración. A la hora de desplegar un mensaje se elige un la etapa de desarrollo del programa: traza, error, error fatal, advertencia, entre otros. Luego con modificar el archivo de configuración se puede fácilmente apagar y prender los niveles para que se impriman o dejen de imprimir algunos mensajes y cambiar rápidamente entre etapas de desarrollo, debugging, pruebas y puesta en producción. Se utilizó la version 1.2.16 de Log4J.

\item Para procesar archivos PDF se utilizó el paquete PDFBox y Tika, en sus versiones 1.6 y 1.1.
\item Para procesar archivos HTML se utilizó el paquete Jericho, en su versión 3.2.
\item Para procesar archivos .doc se utilizó el paquete POI, en su versión 3.8.
\item Para leer archivos de configuración de XML se utilizó el paquete que ofrece Java para leer y consultar XML, JAXP, en su versión 1.4.

\end{itemize}

\section{Implementación del Módulo DOI}\label{sect:implementacion-DOI}

El Determinador del Origen de Incompletitud tiene como objetivo encontrar qué valores de atributos desconocidos son necesarios para poder responder a una consulta y construir un contexto de extracción. Las tareas para poder hacer esto incluyen transformar las condiciones a la forma normal conjuntiva, aplanar las consultas, realizar las consultas sobre la ontología y construir el contexto de extracción. A continuación se expondrán algunas consideraciones relevantes sobre estas tareas. \\ 

\subsection{Transformación de las condiciones a la Forma Normal Conjuntiva}

El procedimiento para transformar cualquier expresión lógica de una condición de una consulta de ODIL en la NCF consiste en realizar una serie de conversiones aplicando algunos teoremas lógicos como las Leyes de De Morgan, reducción de doble negación, propiedades conmutativas y distributivas de los operadores conjunción y disjunción, entre otros. El procedimiento utilizado en este proyecto es una implementación del mecanismo propuesto por el Profesor Jason Eisner en \cite{normalConjunctiveFormImplementation}. Cabe destacar que de acuerdo a Weisstein (1997 en \cite{normalConjunctiveForm}), toda afirmación lógica escrita en términos de conjunciones, negaciones y disjunciones puede ser escrita en la forma normal conjuntiva. \\

\subsection{Implementación del constructor de contextos de extracción}

Para poder construir los contextos de extracción, es necesario en primer lugar aplanar las consultas y las condiciones. El motivo de esto es que el contexto de extracción diseñado para este proyecto, tal y como fue especificado en la sección \ref{sect:diseno-extCont}, consiste en los posibles valores conocidos de los atributos relacionados con el atributo cuyo valor falta. Por lo tanto, el primer paso consiste en aplanar las consultas resolviendo cosas como las consultas anidadas, las operaciones aritméticas presentes en las condiciones, operaciones sobre cadenas de caracteres y en general cualquier cosa que sea necesaria para que en cada consulta sólo haya condiciones. \\

Una vez aplanadas las consultas, se realiza la construcción del contexto de extracción. Para ello, se generan todos los posibles valores de cada atributo relacionado con el atributo original siguiendo la especificación obtenida en un archivo de configuración relacionado con cada dominio de información. Es de interés recordar que dominio de información se refiere al área o contexto de los documentos (designaciones de jurados, designaciones de cargos y ascensos en el escalafón).\\ 

En dicho archivo de configuración se especifica el dominio explícita o implícitamente, enumerando los posibles valores que puede tener o unas cotas superiores e inferiores y un incremento para generar los posibles valores entre ellos, útil para dominios numéricos - enteros y punto flotante - y fechas. Los posibles valores de cada atributo son generados tomando en cuenta esta espeficación y las condiciones de cada consulta.  \\

\section{Implementación del módulo BFI}\label{sect:implementacion-BFI}

Como se ha explicado en la sección \ref{sect:diseno-BFI}, el Buscador Focalizado de Información (BFI) es el módulo que encuentra los valores de los campos que son desconocidos para una consulta dada. Estos valores son buscados en un conjunto de documentos definidos por el usuario y se encuentran tomando en cuenta un contexto de extracción que contiene información útil para la búsqueda. El módulo BFI tiene dos componentes: un preprocesador de texto y un extractor focalizado de información. \\

A grandes rasgos, el funcionamiento y la implementación de ambos componentes son bastantes similares. Ambos componentes operan como extractores de texto utilizando expresiones regulares definidas previamente por un usuario experto. Dichas expresiones regulares funcionan como delimitadores del fragmento de texto que interesa según la tarea que se esté realizando: en el caso del preprocesador de texto, el objetivo es extraer el fragmento de texto que está relacionado con el dominio sobre el cuál se realiza la extracción focalizada; el buscador focalizado encuentra el valor de cada campo faltante para responder una consulta. \\

Ambos componentes están hechos para trabajar con un archivo de configuración en XML que especifica los parámetros necesarios para realizar el preprocesamiento y la extracción. En particular los parámetros que se guardan en estos archivos de configuración incluyen las expresiones regulares, las rutas de directorio de los archivos de entrada, la ruta de los archivos de salida para el caso del preprocesador, entre otras cosas. De esta manera, el usuario del sistema debe preocuparse tan sólo por entender cómo realizar un archivo de configuración para cada uno de los dominios necesarios. \\

Más adelante se expondrán algunas consideraciones sobre las expresiones regulares de interés para el entendimiento de la implementación del BFI. \\

\subsection{Preprocesador de Textos}\label{sect:implementacion-preProcesador}

Ahondando más en el componente de pre-procesamiento, su objetivo es extraer de cada documento el conjunto de fragmentos que está relacionado con el dominio sobre el cual se hace la búsqueda focalizada. En tal sentido tiene como entrada un conjunto de documentos en su formato original (PDF, Portable Readable Document; HTML , Hyper Text Markup Language; DOC y DOCX, archivos de Microsoft Word) y un archivo de configuración. Su salida es un archivo .txt en el cual se tiene el conjunto de fragmentos que están relacionados con el dominio en cuestión según lo especificado por las expresiones regulares. Para procesar los documentos se utilizaron los paquetes de software enumerados al principio de este capítulo. \\

Empíricamente se observó que el procesador de expresiones regulares toma una cantidad de tiempo considerable cuando no logra encontrar una coincidencia. Para poder controlar esto se utilizó un sistema de \emph{timeouts} basado en la utilización de threads, para agilizar el preprocesamiento y la fase de pruebas. \\

\subsection{Extractor Focalizado}\label{sect:implementacion-extractorFocalizado}

En cuanto al componente de extracción focalizada, su objetivo es dado un conjunto de fragmentos de documentos que pueden contener valores de campos necesarios para responder una consulta, encontrar el valor de esos campos. Para ello opera en dos fases: primero separa cada documento en unidades relacionadas con el dominio en cuestión. Para ilustrar este punto, tomando en cuenta los tres dominios que se consideraron para este trabajo, las unidades vienen siendo designaciones, ingresos y ascensos dentro del escalafón y designación de un jurado de ascenso, por trabajo. Este refinamiento sobre los documentos es hecho utilizando expresiones regulares y su objetivo es aislar fragmentos de texto donde haya una cierta cohesión semántica (un fragmento de texto que se refiera a lo mismo) que permita ubicar la respuesta utilizando el contexto de extracción. \\

Una vez separados los documentos en unidades, se utiliza el contexto de extracción para ordenar las unidades en un orden de preferencia. Dicho orden se realiza tomando un valor denominado \emph{UnitHitMeasure} que busca medir a través de una media ponderada cuánto se aproxima o relaciona cada unidad encontrada en los documentos a la unidad correcta según lo especificado por el contexto de extracción. Dicho en otras palabras, se busca ordenar las unidades tomando en cuenta la proporción de valores de campos presentes en cada unidad que coinciden con los valores del contexto de extracción.\\

El \emph{UnitHitMeasure} es calculado de la siguiente manera: cada campo de la ontología tiene un peso que es asignado por un usuario experto en el archivo de configuración. Cada contexto de extracción tendrá uno o más valores de campos conocidos. Para cada campo presente el contexto de extracción cuyo resultado es conocido y que está presente en la unidad se suma el peso asociado a ese campo. Luego se divide esa sumatoria entre la máxima suma posible (el valor de la suma si todos los campos cuyo valor se conocen estuviesen en la unidad con los valores conocidos). De esta manera se obtiene un número del 0 al 1 que indica el grado de relación que tiene la unidad en cuestión con el contexto de extracción focalizado.\\

Por ejemplo, supóngase que se tienen tres campos, C1, C2 y C3; cada uno con pesos de uno, dos y tres respectivamente. Si una unidad hace una correspondencia con el campo C1 y C2, tiene un UHM de .5 ($\frac{(1+2)}{6}$). Si una unidad hace correspondencia con los campos C2 y C3, tiene un UHM de .83 ($\frac{(2+3)}{6}$). \\

El mecanismo señalado al final de la sección \ref{sect:diseno-extCont} para generar todos los posibles estados de las condiciones que se derivan de la forma normal conjuntiva no fue implementado. La forma de implementar de esto consiste en generar una lista de contextos de extracción, cada uno asociado a un posible estado según lo explicado, en lugar de un solo contexto. Luego al hacer la búsqueda focalizada se debe hacer con cada uno de los contextos de extracción y usar un mecanismo para discriminar cuál es la respuesta correcta. \\

Vale acotar que a la hora de determinar si un valor está presente en la unidad se pueden realizar modificaciones de los valores contenidos en la base datos con el objetivo de incorporar el uso de sinónimos, traducción de idiomas, desglose de siglas, entre otros, que constituyen valores equivalentes a los presentes en la base de datos. Por ejemplo, es posible que en la base de datos se tenga el valor \emph{Coord de Ing. Comp.} y que en los documentos aparezca Coordinación de Ingeniería de la Computación. Esto debe ser sin embargo implementado según el contexto y requiere una modificación en los archivos XML y la implementación del código para realizar las modificaciones. Por ejemplo, se implementó una funcionalidad para que dada una fecha en un formato cualquiera, generar los formatos de fechas más comunes según lo utilizado en los documentos. \\

Una vez ordenadas las unidades en orden de precedencia se ubica el valor del campo que se quiere encontrar por medio de expresiones regulares. Cabe destacar que el usuario puede especificar un \emph{Minimum HitMeasure} (valor mínimo del HitMeasure) para que una respuesta sea considerada válida: mientras más cercana a 1 será más probable que la respuesta sea correcta aunque se disminuye la posibilidad de encontrar el valor. \\

\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]%
    {FuncionamientoBFI}
  \caption[Funcionamiento interno del Buscador Focalizado de Información]
   {Funcionamiento interno del Buscador Focalizado de Información}
\end{figure}\label{figura-FuncionamientoBFI}


En la figura \ref{figura-FuncionamientoBFI} puede apreciarse el funcionamiento interno del BFI. \\

\subsection{Diseño de las expresiones regulares}\label{sect:implementacion-regexp}

La elaboración de las expresiones regulares es una tarea que debe realizar un usuario experto que conozca el dominio sobre el cual se quiere realizar la extracción focalizada. En general los dominios admisibles son aquellos con textos estructurados o semi-estructurados, donde se puedan deducir patrones en la redacción de los contenidos. Aún teniendo cierta regularidad en la forma del texto, existen retos como errores de redacción, ortografía, transcripción, formato en los documentos fuente, presencia de tablas entre otros, que dificultan el uso de expresiones regulares. Es posible también por ejemplo la presencia de un conjunto de caracteres que parezcan espacios en blanco pero que no coincidan con una expresión regular porque no soncarácterescarácterescarácteres espacios.carácterescarácterescarácterescarácteres \\carácterescaráctecarácterescarácteresres

En todo caso, lo relevante en esta descripción de la implementación del sistema es que para atender esta dificultad los archivos de XML permiten contener varias expresiones regulares ordenadas por orden de precedencia. Los textos se exploran con las expresiones regulares según el orden de precedencia: si no se encuentra un fragmento de texto con una expresión regular se explora posteriormente con otra de menor precedencia y así sucesivamente. \\

El objetivo de esta precedencia es que el usuario pueda colocar con alta precedencia expresiones regulares que sean bastante selectivas y restrictivas, con el objetivo de asegurar correctitud. Luego a medida que se desciende en la escala de precedencia se puede relajar la restrictividad de la expresión regular, con el objetivo de aumentar la probabilidad de encontrar un fragmento de texto. Relajar la restrictividad implica que la respuesta no sea exactamente la buscada y que por el contrario pueda contener caracteres de más. Esto puede ser deseable en muchos casos, dada la complejidad de utilizar expresiones regulares.  \\

Las expresiones regulares utilizadas funcionan especificando delimitadores anteriores y posteriores al valor que se quiere extraer. Dichos delimitadores son muchos casos cadenas de texto estáticas elegidas por un usuario experto luego de analizar el documento y encontrar repeteciones de frases. En muchos casos se especifican en lugar de cadenas de texto clases de caracteres que especifican un conjunto de símbolos que puede repetirse. La mayoría de las veces se busca combinar ambas posibilidades y que los delimitadores estén basados en las cadenas de texto modeladas con clases de caracteres para minimizar los errores de tipeo, de mayúsculas y minúsculas, diferencias en el formato, etcétera.\\

Para ilustrar el diseño de las expresiones regulares, se ha incluido en el Apéndice \ref{chap:apendiceA} un fragmento de un archivo de configuracion de las expresiones regulares. \\
